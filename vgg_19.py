# -*- coding: utf-8 -*-
"""VGG19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aCJT1VBqykK9bZca7YWpud31nBDEB4OH
"""

!pip install google.colab

import google.colab
google.colab.drive.mount('/content/drive')

!pip install PyDrive googledrivedownloader

from google_drive_downloader import GoogleDriveDownloader

!unzip '/content/drive/MyDrive/test.zip' -d '/content/drive/MyDrive/OCTfiles'

!unzip '/content/drive/MyDrive/val.zip' -d '/content/drive/MyDrive/OCTfiles'

!apt install unzip

!mkdir '/content/drive/MyDrive/OCTfiles'

!unzip '/content/drive/MyDrive/train.zip' -d '/content/drive/MyDrive/OCTfiles'

!pip install tensorflow_addons

import os
import random
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow_addons as tfa
import tensorflow as tf
import numpy as np
import sklearn.metrics
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img

base_dir = os.path.join("/content/drive/MyDrive/OCTfiles")
print('Base directory --> ', os.listdir(base_dir))

train_dir = os.path.join(base_dir + "/train/")
print("Train Directory --> ", os.listdir(train_dir))

validation_dir = os.path.join(base_dir + "/val/")
print("Validation Directory --> ", os.listdir(validation_dir))

test_dir = os.path.join(base_dir + "/test/")
print("Test Directory --> ", os.listdir(test_dir))

vgg19 = tf.keras.applications.VGG19(
    include_top = False, 
    weights = 'imagenet', 
    input_tensor = None,
    input_shape = (224,224,3), 
    pooling = None, 
    classes = 1000
)

vgg19.trainable = False

model_vgg = tf.keras.models.Sequential([
    
    vgg19,
    tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = 'same'),
    tf.keras.layers.PReLU(alpha_initializer='zeros'),
    tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = 'same'),
    tf.keras.layers.PReLU(alpha_initializer='zeros'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(100),
    tf.keras.layers.PReLU(alpha_initializer='zeros'),
    tf.keras.layers.Dense(4, activation = 'softmax')
])

metrics = ['accuracy',
                tf.keras.metrics.AUC(),
                tfa.metrics.CohenKappa(num_classes = 4),
                tfa.metrics.F1Score(num_classes = 4),
                tf.keras.metrics.Precision(), 
                tf.keras.metrics.Recall()]

model_vgg.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = metrics)
model_vgg.summary()

train_datagen = ImageDataGenerator(rescale = 1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size = (224, 224), class_mode = 'categorical', batch_size = 100)

validation_datagen = ImageDataGenerator(rescale = 1./255)
validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size = (224, 224), class_mode = 'categorical', batch_size = 16)

test_datagen = ImageDataGenerator(rescale = 1./255)
test_generator = test_datagen.flow_from_directory(test_dir, target_size = (224, 224), class_mode = 'categorical', batch_size = 50)

!nvidia-smi

history_vgg = model_vgg.fit(
    train_generator,
    steps_per_epoch = (83484/100),
    epochs = 10,
    validation_data = validation_generator,
    validation_steps = (32/16),
    max_queue_size=100,
    workers = 4 ,
    use_multiprocessing=True,
    verbose = 1)

model_vgg.save("/content/drive/MyDrive/vgg19_model_2")

from keras.models import load_model

model = load_model("/content/drive/My Drive/vgg19_model_2")
model.summary()

print("Values for VGG-19 based ConvNet")
acc = history_vgg.history['accuracy']
val_acc = history_vgg.history['val_accuracy']
loss = history_vgg.history['loss']
val_loss = history_vgg.history['val_loss']

epochs = range(len(acc))

plt.figure(figsize=(12,12))

plt.plot(epochs, acc, 'dodgerblue', label = 'Training accuracy')
plt.plot(epochs, val_acc, 'aquamarine', label = 'Validation accuracy')
plt.title('Training & validation accuracy')
plt.legend()

plt.figure(figsize = (12,12))

plt.plot(epochs, loss, 'dodgerblue', label = 'Training Loss')
plt.plot(epochs, val_loss, 'aquamarine', label = 'Validation Loss')
plt.title('Training $ validation loss')
plt.legend()

plt.show()

model_vgg.evaluate(test_generator)

test_steps_per_epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)

predictions = model_vgg.predict_generator(test_generator, steps = test_steps_per_epoch)

predicted_classes = np.argmax(predictions, axis=1)

true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

report = sklearn.metrics.classification_report(true_classes, predicted_classes, target_names = class_labels)
print(report)

cm = sklearn.metrics.confusion_matrix(true_classes, predicted_classes)
plt.figure(figsize=(8,8))
sb.heatmap(cm, fmt='.0f', cmap="YlGnBu",annot=True, linewidths=0.2, linecolor='black')
plt.xlabel('predicted value')
plt.ylabel('Truth value')
plt.show()