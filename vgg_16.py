# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBubyhqtwe9NuA-0OFqD2P619sTKxnH1
"""

!pip install tensorflow_addons

# Load the Drive helper and mount
from google.colab import drive
# This will prompt for authorization.
drive.mount('/content/drive')

!ls "/content/drive/My Drive/Colab Notebooks"
!ls "/content"

#unzip the train set data
!jar xvf "/content/train.zip"

import os
import random

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.metrics

import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img

base_dir = os.path.join("/content/")
print('Base directory --> ', os.listdir(base_dir))

train_dir = os.path.join(base_dir + "train/")
print("Train Directory --> ", os.listdir(train_dir))

validation_dir = os.path.join(base_dir + "/drive/My Drive/Colab Notebooks/val/val")
print("Validation Directory --> ", os.listdir(validation_dir))

test_dir = os.path.join(base_dir + "/drive/My Drive/Colab Notebooks/test/test")
print("Test Directory --> ", os.listdir(test_dir))

#build vgg16 model
vgg16 = tf.keras.applications.VGG16(
    include_top = False, 
    weights = 'imagenet', 
    input_tensor = None,
    input_shape = (224,224,3), 
    pooling = max, 
    classes = 1000
)

#freeze the pretrained model parameters
vgg16.trainable = False

# add more layers to vgg16, set the output classes to 4

model = tf.keras.models.Sequential([
    
    vgg16,
    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),
    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(100, activation = 'relu'),
    tf.keras.layers.Dense(4, activation = 'softmax')
])

model.summary()
vgg16.summary()

metrics_list = ['accuracy',
                tf.keras.metrics.AUC(),
                tfa.metrics.CohenKappa(num_classes = 4),
                tfa.metrics.F1Score(num_classes = 4)]

model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = metrics_list)

train_datagen = ImageDataGenerator(rescale = 1./255)
train_generator = train_datagen.flow_from_directory(train_dir, target_size = (224, 224), class_mode = 'categorical', batch_size = 100)

validation_datagen = ImageDataGenerator(rescale = 1./255)
validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size = (224, 224), class_mode = 'categorical', batch_size = 16)

test_datagen = ImageDataGenerator(rescale = 1./255)
test_generator = test_datagen.flow_from_directory(test_dir, target_size = (224, 224), class_mode = 'categorical', batch_size = 50)

history = model.fit_generator(
    train_generator,
    steps_per_epoch = (83484/100),
    epochs = 10,
    validation_data = validation_generator,
    validation_steps = (32/16),
    verbose = 1)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.figure(figsize=(7,7))

plt.plot(epochs, acc, 'r', label = 'Training accuracy')
plt.plot(epochs, val_acc, 'b', label = 'Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure(figsize = (7,7))

plt.plot(epochs, loss, 'r', label = 'Training Loss')
plt.plot(epochs, val_loss, 'b', label = 'Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

model.predict(test_generator, steps = int(968/50))

model.evaluate(test_generator)

#new try it!
test_steps_per_epoch = np.math.ceil(test_generator.samples / test_generator.batch_size)

predictions = model_vgg.predict_generator(test_generator, steps = test_steps_per_epoch)

predicted_classes = np.argmax(predictions, axis=1)